# 🧠 COMPLETE ATTENTION MECHANISMS VISUAL GUIDE
## Every Figure from Chapter 3 Explained in Detail
### "Build a Large Language Model From Scratch" - Sebastian Raschka

---

# 📑 TABLE OF CONTENTS

| Section | Figures | Topic |
|---------|---------|-------|
| 1 | 3.1-3.2 | Big Picture & Roadmap |
| 2 | 3.4-3.6 | Why Attention? (RNN Problem) |
| 3 | 3.7-3.12 | Simplified Self-Attention |
| 4 | 3.13-3.18 | Self-Attention with Trainable Weights |
| 5 | 3.19-3.23 | Causal Attention |
| 6 | 3.24-3.26 | Multi-Head Attention |
| 7 | - | Complete Code & Summary |

---

# SECTION 1: THE BIG PICTURE

## 📊 Figure 3.1: Where Attention Fits in Building an LLM

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                           BUILDING AN LLM                                        │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║                          STAGE 1: Foundation Model                         ║ │
│  ╠═══════════════════════════════════════════════════════════════════════════╣ │
│  ║                                                                           ║ │
│  ║  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐   ║ │
│  ║  │ 1) Data     │   │ 2) ATTENTION│   │ 3) LLM      │   │ 4) Pre-     │   ║ │
│  ║  │ Preparation │──▶│ MECHANISM   │──▶│ Architecture│──▶│ training    │   ║ │
│  ║  │ & Sampling  │   │ ◀══════════ │   │             │   │             │   ║ │
│  ║  └─────────────┘   │ WE ARE HERE │   └─────────────┘   └─────────────┘   ║ │
│  ║                    └─────────────┘                                        ║ │
│  ║                           │                                               ║ │
│  ║                           │                                               ║ │
│  ║  ┌─────────────┐   ┌──────▼──────┐   ┌─────────────┐                      ║ │
│  ║  │ 5) Training │   │ 6) Model    │   │ 7) Load     │                      ║ │
│  ║  │ Loop        │◀──│ Evaluation  │◀──│ Pretrained  │                      ║ │
│  ║  │             │   │             │   │ Weights     │                      ║ │
│  ║  └─────────────┘   └─────────────┘   └─────────────┘                      ║ │
│  ║                                                                           ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  STAGE 2: Classifier          ║  STAGE 3: Personal Assistant              ║ │
│  ║  (Dataset with class labels)  ║  (Instruction dataset)                    ║ │
│  ║                               ║                                           ║ │
│  ║  ┌─────────────┐              ║  ┌─────────────┐                          ║ │
│  ║  │ 8) Fine-    │              ║  │ 9) Fine-    │                          ║ │
│  ║  │ tuning      │              ║  │ tuning      │                          ║ │
│  ║  └─────────────┘              ║  └─────────────┘                          ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘

KEY INSIGHT: Attention mechanism is the CORE building block of GPT-like LLMs.
             Without understanding attention, you can't understand transformers!
```

---

## 📊 Figure 3.2: The Four Attention Variants (Our Roadmap)

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    THE FOUR ATTENTION MECHANISMS WE WILL BUILD                   │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│   ┌───────────────┐      ┌───────────────┐      ┌───────────────┐      ┌───────────────┐
│   │               │      │               │      │               │      │               │
│   │  1) SIMPLIFIED│ ───▶ │ 2) SELF-ATTN  │ ───▶ │  3) CAUSAL    │ ───▶ │ 4) MULTI-HEAD │
│   │  SELF-ATTN    │      │ WITH WEIGHTS  │      │  ATTENTION    │      │  ATTENTION    │
│   │               │      │               │      │               │      │               │
│   └───────────────┘      └───────────────┘      └───────────────┘      └───────────────┘
│          │                      │                      │                      │
│          ▼                      ▼                      ▼                      ▼
│   ┌───────────────┐      ┌───────────────┐      ┌───────────────┐      ┌───────────────┐
│   │ A simplified  │      │ Self-attention│      │ Adds a mask   │      │ Multiple      │
│   │ version to    │      │ with trainable│      │ to prevent    │      │ attention     │
│   │ introduce the │      │ weights (W_q, │      │ seeing future │      │ heads in      │
│   │ broader idea  │      │ W_k, W_v)     │      │ tokens during │      │ parallel for  │
│   │               │      │               │      │ text gen      │      │ different     │
│   │ NO training   │      │ Used in real  │      │               │      │ patterns      │
│   │ weights       │      │ LLMs          │      │ Essential for │      │               │
│   │               │      │               │      │ GPT models    │      │ FINAL VERSION │
│   └───────────────┘      └───────────────┘      └───────────────┘      └───────────────┘
│                                                                                 │
│   DIFFICULTY:  ⭐             ⭐⭐             ⭐⭐⭐            ⭐⭐⭐⭐         │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘

Each builds on the previous one!
```

---

# SECTION 2: WHY ATTENTION? (THE RNN PROBLEM)

## 📊 Figure 3.4: The Problem with Encoder-Decoder RNNs

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│              BEFORE ATTENTION: ENCODER-DECODER RNN                               │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  TASK: Translate German → English                                               │
│                                                                                 │
│  INPUT: "Kannst du mir helfen?"  (German)                                       │
│  OUTPUT: "Can you help me?"      (English)                                      │
│                                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                           ENCODER                                        │   │
│  │                                                                         │   │
│  │   "Kannst"    "du"      "mir"     "helfen"    "?"                       │   │
│  │      │         │          │          │         │                        │   │
│  │      ▼         ▼          ▼          ▼         ▼                        │   │
│  │   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐   ┌─────┐                       │   │
│  │   │ h₁  │──▶│ h₂  │──▶│ h₃  │──▶│ h₄  │──▶│ h₅  │                       │   │
│  │   └─────┘   └─────┘   └─────┘   └─────┘   └─────┘                       │   │
│  │                                              │                          │   │
│  │                                              │  ENTIRE sentence         │   │
│  │                                              │  compressed into         │   │
│  │                                              │  ONE vector!             │   │
│  │                                              │                          │   │
│  │                                              ▼                          │   │
│  │                                       ╔═══════════╗                     │   │
│  │                                       ║  MEMORY   ║ ◀── BOTTLENECK!     │   │
│  │                                       ║   CELL    ║                     │   │
│  │                                       ╚═════╦═════╝                     │   │
│  │                                             ║                           │   │
│  └─────────────────────────────────────────────║───────────────────────────┘   │
│                                                ║                               │
│  ┌─────────────────────────────────────────────║───────────────────────────┐   │
│  │                           DECODER           ▼                            │   │
│  │                                                                         │   │
│  │                                       ┌─────┐                            │   │
│  │                                       │ h₁' │                            │   │
│  │                                       └──┬──┘                            │   │
│  │                                          │                              │   │
│  │                                          ▼                              │   │
│  │                                        "Can"                            │   │
│  │                                          │                              │   │
│  │                                          ▼                              │   │
│  │                                       ┌─────┐                            │   │
│  │                                       │ h₂' │──▶ "you" ──▶ ...          │   │
│  │                                       └─────┘                            │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  THE PROBLEM:                                                             ║ │
│  ║  ──────────────                                                           ║ │
│  ║  • The decoder can ONLY access the FINAL hidden state                     ║ │
│  ║  • The entire input must be compressed into ONE vector                    ║ │
│  ║  • Long sentences → LOSS OF CONTEXT                                       ║ │
│  ║  • Early words get "forgotten" as more words are processed                ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.5: The Attention Solution (Bahdanau Attention)

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    THE SOLUTION: ATTENTION MECHANISM                             │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  TASK: Translate German → English                                               │
│  Currently generating: "help" (the 3rd output word)                             │
│                                                                                 │
│  INPUT TOKENS:                                                                  │
│                                                                                 │
│     "Kannst"      "du"        "mir"       "helfen"       "?"                   │
│        │           │            │            │            │                     │
│        │           │            │            │            │                     │
│        ▼           ▼            ▼            ▼            ▼                     │
│     ┌─────┐     ┌─────┐     ┌─────┐     ┌─────┐     ┌─────┐                    │
│     │ h₁  │     │ h₂  │     │ h₃  │     │ h₄  │     │ h₅  │  Hidden states    │
│     └──┬──┘     └──┬──┘     └──┬──┘     └──┬──┘     └──┬──┘                    │
│        │           │            │            │            │                     │
│        │           │            │            │            │                     │
│        │   ........│............│............│............│                     │
│        │   :       │    :       │      :     │      :     │                     │
│        │   :       │    :       │      :     │      :     │                     │
│        │   :       │    :       │      :     │      :     │                     │
│        ▼   ▼       ▼    ▼       ▼      ▼     ▼      ▼     ▼                     │
│     ╔══════════════════════════════════════════════════════════╗               │
│     ║              ATTENTION WEIGHTS                           ║               │
│     ║                                                          ║               │
│     ║   0.05       0.10        0.15        0.60        0.10    ║               │
│     ║    │          │           │            │           │     ║               │
│     ║    │          │           │           ████         │     ║  Line width   │
│     ║    │          │           │           ████         │     ║  shows        │
│     ║    ·          ·           │           ████         ·     ║  importance!  │
│     ║                           │           ████               ║               │
│     ╚═══════════════════════════╪═══════════════════════════════╝               │
│                                 │                                               │
│                                 ▼                                               │
│                           ┌───────────┐                                         │
│                           │  DECODER  │                                         │
│                           │ (h₃')     │                                         │
│                           └─────┬─────┘                                         │
│                                 │                                               │
│                                 ▼                                               │
│                              "help"  ◀── OUTPUT TOKEN                           │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  THE SOLUTION:                                                            ║ │
│  ║  ─────────────                                                            ║ │
│  ║  • Decoder can ACCESS ALL input hidden states                             ║ │
│  ║  • Attention weights determine which inputs are IMPORTANT                 ║ │
│  ║  • For "help", the model focuses on "helfen" (highest weight: 0.60)       ║ │
│  ║  • No information bottleneck!                                             ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.6: Self-Attention in GPT-like Transformers

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    GPT-LIKE DECODER-ONLY TRANSFORMER                             │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│                              Input Text                                         │
│                                  │                                              │
│                                  ▼                                              │
│                    ┌─────────────────────────────┐                              │
│                    │    PREPROCESSING STEPS      │  ◀── Previous chapter       │
│                    │    (Tokenization, etc.)     │      (Chapter 2)            │
│                    └──────────────┬──────────────┘                              │
│                                   │                                              │
│                                   ▼                                              │
│    ╔══════════════════════════════════════════════════════════════════════╗    │
│    ║                                                                      ║    │
│    ║              ┌─────────────────────────────┐                         ║    │
│    ║              │                             │                         ║    │
│    ║              │   SELF-ATTENTION MODULE     │  ◀══ THIS CHAPTER!      ║    │
│    ║              │                             │                         ║    │
│    ║              │   • Each position attends   │                         ║    │
│    ║              │     to ALL other positions  │                         ║    │
│    ║              │                             │                         ║    │
│    ║              │   • Computes importance     │                         ║    │
│    ║              │     weights automatically   │                         ║    │
│    ║              │                             │                         ║    │
│    ║              └─────────────────────────────┘                         ║    │
│    ║                                                                      ║    │
│    ╚══════════════════════════════════════════════════════════════════════╝    │
│                                   │                                              │
│                                   ▼                                              │
│                    ┌─────────────────────────────┐                              │
│                    │  THE REMAINING PARTS OF     │  ◀── Next chapter           │
│                    │  THE LLM ARCHITECTURE       │      (Chapter 4)            │
│                    │  (Feed-forward layers, etc.)│                              │
│                    └──────────────┬──────────────┘                              │
│                                   │                                              │
│                                   ▼                                              │
│                    ┌─────────────────────────────┐                              │
│                    │    POSTPROCESSING STEPS     │                              │
│                    │    (Sampling, decoding)     │                              │
│                    └──────────────┬──────────────┘                              │
│                                   │                                              │
│                                   ▼                                              │
│                              Output Text                                        │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘

"SELF" in self-attention means:
────────────────────────────────
• The sequence attends to ITSELF (not to another sequence)
• Each word looks at ALL words in the SAME sentence
• Learns relationships WITHIN the input
```

---

# SECTION 3: SIMPLIFIED SELF-ATTENTION

## 📊 Figure 3.7: The Goal - Computing Context Vectors

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    GOAL OF SELF-ATTENTION                                        │
│                    ─────────────────────                                         │
│  Compute a CONTEXT VECTOR for each input that combines info from ALL inputs     │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  INPUT SENTENCE: "Your journey starts with one step"                            │
│                                                                                 │
│  INPUT VECTORS (token embeddings):                                              │
│                                                                                 │
│       x⁽¹⁾          x⁽²⁾          x⁽³⁾          x⁽⁴⁾          x⁽⁵⁾          x⁽⁶⁾    │
│      "Your"      "journey"     "starts"      "with"        "one"        "step"  │
│     ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐ │
│     │ 0.43  │    │ 0.55  │    │ 0.57  │    │ 0.22  │    │ 0.77  │    │ 0.05  │ │
│     │ 0.15  │    │ 0.87  │    │ 0.85  │    │ 0.58  │    │ 0.25  │    │ 0.80  │ │
│     │ 0.89  │    │ 0.66  │    │ 0.64  │    │ 0.33  │    │ 0.10  │    │ 0.55  │ │
│     └───┬───┘    └───┬───┘    └───┬───┘    └───┬───┘    └───┬───┘    └───┬───┘ │
│         │            │            │            │            │            │      │
│         │     ╔══════╧══════╗     │            │            │            │      │
│         │     ║   QUERY     ║     │            │            │            │      │
│         │     ║ We want to  ║     │            │            │            │      │
│         │     ║ compute z⁽²⁾║     │            │            │            │      │
│         │     ╚══════╤══════╝     │            │            │            │      │
│         │            │            │            │            │            │      │
│         │            │            │            │            │            │      │
│         │    α₂₁     │   α₂₂      │   α₂₃      │   α₂₄      │   α₂₅      │  α₂₆ │
│         │◀───────────┤◀───────────┤◀───────────┤◀───────────┤◀───────────┤◀────│ │
│         │   0.14     │   0.24     │   0.23     │   0.12     │   0.11     │ 0.16 │
│         │            │            │            │            │            │      │
│         │            │            │            │            │            │      │
│         └────────────┴────────────┴────────────┴────────────┴────────────┘      │
│                                   │                                              │
│                    ATTENTION WEIGHTS (sum to 1.0)                               │
│                    determine importance of each input                           │
│                                   │                                              │
│                                   ▼                                              │
│                            ┌───────────┐                                        │
│                            │   0.44    │                                        │
│                   z⁽²⁾ =  │   0.65    │  CONTEXT VECTOR                        │
│                            │   0.57    │  (enriched representation)             │
│                            └───────────┘                                        │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  WHAT IS A CONTEXT VECTOR?                                                ║ │
│  ║  ─────────────────────────                                                ║ │
│  ║  • An "enriched" embedding that contains info from ALL other words        ║ │
│  ║  • z⁽²⁾ for "journey" knows about "Your", "starts", "with", "one", "step" ║ │
│  ║  • This lets the model understand word RELATIONSHIPS                      ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.8: Step 1 - Computing Attention Scores (Dot Product)

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    STEP 1: COMPUTE ATTENTION SCORES                              │
│                    ────────────────────────────────                              │
│                    Using DOT PRODUCT to measure similarity                       │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  QUERY: x⁽²⁾ = "journey" = [0.55, 0.87, 0.66]                                   │
│                                                                                 │
│  We compute: query · each_input = attention_score                               │
│                                                                                 │
│                                                                                 │
│       x⁽¹⁾          x⁽²⁾          x⁽³⁾          x⁽⁴⁾          x⁽⁵⁾          x⁽⁶⁾    │
│      "Your"      "journey"     "starts"      "with"        "one"        "step"  │
│     ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐ │
│     │ 0.4   │    │ 0.5   │    │ 0.5   │    │ 0.2   │    │ 0.8   │    │ 0.0   │ │
│     │ 0.1   │    │ 0.8   │    │ 0.8   │    │ 0.6   │    │ 0.2   │    │ 0.8   │ │
│     │ 0.8   │    │ 0.6   │    │ 0.6   │    │ 0.3   │    │ 0.1   │    │ 0.5   │ │
│     └───┬───┘    └───┬───┘    └───┬───┘    └───┬───┘    └───┬───┘    └───┬───┘ │
│         │            │            │            │            │            │      │
│         │    QUERY   │            │            │            │            │      │
│         │    [0.5,   │            │            │            │            │      │
│         │     0.8,   │            │            │            │            │      │
│         │     0.6]   │            │            │            │            │      │
│         │            │            │            │            │            │      │
│         ▼            ▼            ▼            ▼            ▼            ▼      │
│       x⁽¹⁾·q       x⁽²⁾·q       x⁽³⁾·q       x⁽⁴⁾·q       x⁽⁵⁾·q       x⁽⁶⁾·q    │
│         │            │            │            │            │            │      │
│         ▼            ▼            ▼            ▼            ▼            ▼      │
│     ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐ │
│     │ ω₂₁   │    │ ω₂₂   │    │ ω₂₃   │    │ ω₂₄   │    │ ω₂₅   │    │ ω₂₆   │ │
│     │ =0.95 │    │ =1.49 │    │ =1.48 │    │ =0.84 │    │ =0.71 │    │ =1.09 │ │
│     └───────┘    └───────┘    └───────┘    └───────┘    └───────┘    └───────┘ │
│                                                                                 │
│         ▲            ▲                                                          │
│         │            │                                                          │
│         │        HIGHEST SCORE                                                  │
│         │        (most similar to query)                                        │
│         │                                                                       │
│      Lower score                                                                │
│      (less similar)                                                             │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  DOT PRODUCT CALCULATION:                                                 ║ │
│  ║  ────────────────────────                                                 ║ │
│  ║                                                                           ║ │
│  ║  x⁽¹⁾ · query = [0.4, 0.1, 0.8] · [0.5, 0.8, 0.6]                        ║ │
│  ║               = (0.4 × 0.5) + (0.1 × 0.8) + (0.8 × 0.6)                   ║ │
│  ║               = 0.20 + 0.08 + 0.48                                        ║ │
│  ║               = 0.76 ≈ 0.95                                               ║ │
│  ║                                                                           ║ │
│  ║  WHY DOT PRODUCT?                                                         ║ │
│  ║  • Higher dot product = more aligned vectors = more SIMILAR               ║ │
│  ║  • Similar words should have HIGHER attention!                            ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  query = inputs[1]  # "journey"                                                 │
│  attn_scores_2 = torch.empty(inputs.shape[0])                                   │
│  for i, x_i in enumerate(inputs):                                               │
│      attn_scores_2[i] = torch.dot(x_i, query)                                   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.9: Step 2 - Normalizing to Attention Weights

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    STEP 2: NORMALIZE WITH SOFTMAX                                │
│                    ──────────────────────────────                                │
│                    Convert scores to weights that sum to 1                       │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  INPUT: Attention Scores (ω)                                                    │
│                                                                                 │
│     ω₂₁         ω₂₂         ω₂₃         ω₂₄         ω₂₅         ω₂₆            │
│     0.95        1.49        1.48        0.84        0.71        1.09           │
│      │           │           │           │           │           │              │
│      │           │           │           │           │           │              │
│      └───────────┴───────────┴───────────┴───────────┴───────────┘              │
│                              │                                                  │
│                              │                                                  │
│                              ▼                                                  │
│                    ┌─────────────────────┐                                      │
│                    │                     │                                      │
│                    │      SOFTMAX        │                                      │
│                    │                     │                                      │
│                    │         e^ωᵢ        │                                      │
│                    │  αᵢ = ─────────     │                                      │
│                    │        Σ e^ωⱼ       │                                      │
│                    │                     │                                      │
│                    └─────────┬───────────┘                                      │
│                              │                                                  │
│                              ▼                                                  │
│                                                                                 │
│  OUTPUT: Attention Weights (α)                                                  │
│                                                                                 │
│     α₂₁         α₂₂         α₂₃         α₂₄         α₂₅         α₂₆            │
│     0.14        0.24        0.23        0.12        0.11        0.16           │
│      │           │           │           │           │           │              │
│      │           │           │           │           │           │              │
│      └───────────┴───────────┴───────────┴───────────┴───────────┘              │
│                              │                                                  │
│                              ▼                                                  │
│                         SUM = 1.00                                              │
│                                                                                 │
│                                                                                 │
│  VISUAL COMPARISON:                                                             │
│                                                                                 │
│  BEFORE (scores):           AFTER (weights):                                    │
│                                                                                 │
│    1.49 ████████████         0.24 ████████                                      │
│    1.48 ███████████          0.23 ████████                                      │
│    1.09 ███████              0.16 █████                                         │
│    0.95 █████                0.14 ████                                          │
│    0.84 ████                 0.12 ████                                          │
│    0.71 ███                  0.11 ███                                           │
│                                                                                 │
│    (raw scores)              (percentages)                                      │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  WHY SOFTMAX?                                                             ║ │
│  ║  ────────────                                                             ║ │
│  ║  1. All values become POSITIVE                                            ║ │
│  ║  2. All values SUM TO 1 (like probabilities)                              ║ │
│  ║  3. Higher scores get proportionally HIGHER weights (exponential)         ║ │
│  ║  4. Better GRADIENT properties during training                            ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  attn_weights_2 = torch.softmax(attn_scores_2, dim=0)                          │
│  print("Sum:", attn_weights_2.sum())  # tensor(1.0000)                         │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.10: Step 3 - Computing the Context Vector

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    STEP 3: COMPUTE CONTEXT VECTOR                                │
│                    ──────────────────────────────                                │
│                    Weighted sum of all input vectors                             │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  INPUTS:                                                                        │
│                                                                                 │
│       x⁽¹⁾          x⁽²⁾          x⁽³⁾          x⁽⁴⁾          x⁽⁵⁾          x⁽⁶⁾    │
│     ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐ │
│     │ 0.43  │    │ 0.55  │    │ 0.57  │    │ 0.22  │    │ 0.77  │    │ 0.05  │ │
│     │ 0.15  │    │ 0.87  │    │ 0.85  │    │ 0.58  │    │ 0.25  │    │ 0.80  │ │
│     │ 0.89  │    │ 0.66  │    │ 0.64  │    │ 0.33  │    │ 0.10  │    │ 0.55  │ │
│     └───────┘    └───────┘    └───────┘    └───────┘    └───────┘    └───────┘ │
│                                                                                 │
│  ATTENTION WEIGHTS:                                                             │
│                                                                                 │
│       α₂₁          α₂₂          α₂₃          α₂₄          α₂₅          α₂₆      │
│        ×            ×            ×            ×            ×            ×       │
│       0.14         0.24         0.23         0.12         0.11         0.16    │
│                                                                                 │
│         │            │            │            │            │            │      │
│         ▼            ▼            ▼            ▼            ▼            ▼      │
│                                                                                 │
│  WEIGHTED INPUTS:                                                               │
│                                                                                 │
│     ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐ │
│     │ 0.06  │    │ 0.13  │    │ 0.13  │    │ 0.03  │    │ 0.08  │    │ 0.01  │ │
│     │ 0.02  │    │ 0.21  │    │ 0.20  │    │ 0.07  │    │ 0.03  │    │ 0.13  │ │
│     │ 0.12  │    │ 0.16  │    │ 0.15  │    │ 0.04  │    │ 0.01  │    │ 0.09  │ │
│     └───┬───┘    └───┬───┘    └───┬───┘    └───┬───┘    └───┬───┘    └───┬───┘ │
│         │            │            │            │            │            │      │
│         │            │            │            │            │            │      │
│         └────────────┴────────────┴────────────┴────────────┴────────────┘      │
│                                   │                                              │
│                                   │  SUM                                         │
│                                   │                                              │
│                                   ▼                                              │
│                                                                                 │
│                            ┌─────────────┐                                      │
│                            │             │                                      │
│                            │    0.44     │                                      │
│                   z⁽²⁾ =  │    0.65     │   CONTEXT VECTOR                     │
│                            │    0.57     │                                      │
│                            │             │                                      │
│                            └─────────────┘                                      │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  FORMULA:                                                                 ║ │
│  ║  ────────                                                                 ║ │
│  ║                                                                           ║ │
│  ║  z⁽²⁾ = α₂₁·x⁽¹⁾ + α₂₂·x⁽²⁾ + α₂₃·x⁽³⁾ + α₂₄·x⁽⁴⁾ + α₂₅·x⁽⁵⁾ + α₂₆·x⁽⁶⁾   ║ │
│  ║                                                                           ║ │
│  ║  z⁽²⁾ = 0.14·[0.43,0.15,0.89] + 0.24·[0.55,0.87,0.66] + ...              ║ │
│  ║                                                                           ║ │
│  ║  z⁽²⁾ = [0.44, 0.65, 0.57]                                                ║ │
│  ║                                                                           ║ │
│  ║  The context vector is the weighted AVERAGE of all inputs!                ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  context_vec_2 = torch.zeros(query.shape)                                       │
│  for i, x_i in enumerate(inputs):                                               │
│      context_vec_2 += attn_weights_2[i] * x_i                                   │
│  # Result: tensor([0.4419, 0.6515, 0.5683])                                     │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.11: Attention Weight Matrix for All Tokens

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    ATTENTION WEIGHTS FOR ALL TOKENS                              │
│                    ────────────────────────────────                              │
│                    Each row shows attention for one query                        │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│                           KEYS (what we attend TO)                              │
│                    ┌─────────────────────────────────────────────┐              │
│                    │  Your  journey starts  with   one   step   │              │
│      QUERIES       │   ↓       ↓       ↓      ↓      ↓      ↓   │              │
│    (what attends)  ├─────────────────────────────────────────────┤              │
│                    │                                             │              │
│         Your    →  │  0.21   0.20   0.20   0.12   0.12   0.15   │  = 1.00      │
│                    │                                             │              │
│      journey    →  │  0.14   0.24   0.23   0.12   0.11   0.16   │  = 1.00      │
│                    │   │       │                                 │              │
│       starts    →  │  0.14   0.24   0.23   0.12   0.11   0.16   │  = 1.00      │
│                    │                                             │              │
│         with    →  │  0.14   0.21   0.20   0.15   0.13   0.17   │  = 1.00      │
│                    │                                             │              │
│          one    →  │  0.15   0.20   0.20   0.14   0.19   0.13   │  = 1.00      │
│                    │                                             │              │
│         step    →  │  0.14   0.22   0.21   0.14   0.10   0.19   │  = 1.00      │
│                    │                                             │              │
│                    └─────────────────────────────────────────────┘              │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  HOW TO READ THIS MATRIX:                                                 ║ │
│  ║  ────────────────────────                                                 ║ │
│  ║                                                                           ║ │
│  ║  • Row 2 ("journey"): When computing z⁽²⁾, how much attention            ║ │
│  ║    does "journey" pay to each word?                                       ║ │
│  ║                                                                           ║ │
│  ║  • "journey" pays 0.24 (24%) attention to itself                         ║ │
│  ║  • "journey" pays 0.23 (23%) attention to "starts" (similar embedding)   ║ │
│  ║  • "journey" pays 0.11 (11%) attention to "one" (least similar)          ║ │
│  ║                                                                           ║ │
│  ║  • Each ROW sums to 1.0 (100%)                                           ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  # Compute ALL at once with matrix multiplication!                              │
│  attn_scores = inputs @ inputs.T          # (6, 6) matrix                      │
│  attn_weights = torch.softmax(attn_scores, dim=-1)                             │
│  all_context_vecs = attn_weights @ inputs # (6, 3) - all context vectors!     │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.12: The Three Steps Summary

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    SIMPLIFIED SELF-ATTENTION: 3 STEPS                            │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                         │   │
│  │   STEP 1: COMPUTE ATTENTION SCORES                                      │   │
│  │   ────────────────────────────────                                      │   │
│  │                                                                         │   │
│  │   • Compute dot products between all pairs of inputs                    │   │
│  │   • Higher dot product = more similar                                   │   │
│  │                                                                         │   │
│  │   attn_scores = inputs @ inputs.T                                       │   │
│  │                                                                         │   │
│  │   Shape: (6, 3) @ (3, 6) = (6, 6)                                       │   │
│  │                                                                         │   │
│  └────────────────────────────────┬────────────────────────────────────────┘   │
│                                   │                                             │
│                                   ▼                                             │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                         │   │
│  │   STEP 2: COMPUTE ATTENTION WEIGHTS                                     │   │
│  │   ─────────────────────────────────                                     │   │
│  │                                                                         │   │
│  │   • Normalize scores with softmax                                       │   │
│  │   • Each row sums to 1.0                                                │   │
│  │                                                                         │   │
│  │   attn_weights = torch.softmax(attn_scores, dim=-1)                     │   │
│  │                                                                         │   │
│  │   Shape: (6, 6) → (6, 6)                                                │   │
│  │                                                                         │   │
│  └────────────────────────────────┬────────────────────────────────────────┘   │
│                                   │                                             │
│                                   ▼                                             │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                         │   │
│  │   STEP 3: COMPUTE CONTEXT VECTORS                                       │   │
│  │   ───────────────────────────────                                       │   │
│  │                                                                         │   │
│  │   • Weighted sum of inputs                                              │   │
│  │   • Each token gets enriched context                                    │   │
│  │                                                                         │   │
│  │   all_context_vecs = attn_weights @ inputs                              │   │
│  │                                                                         │   │
│  │   Shape: (6, 6) @ (6, 3) = (6, 3)                                       │   │
│  │                                                                         │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  SUMMARY IN ONE LINE:                                                     ║ │
│  ║                                                                           ║ │
│  ║  context = softmax(X @ X.T) @ X                                          ║ │
│  ║                                                                           ║ │
│  ║  But this is TOO SIMPLE - no trainable weights!                          ║ │
│  ║  Let's add them in the next section...                                   ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

# SECTION 4: SELF-ATTENTION WITH TRAINABLE WEIGHTS

## 📊 Figure 3.13: Progress - Adding Trainable Weights

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    OUR PROGRESS                                                  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│   ┌───────────────┐      ┌───────────────┐      ┌───────────────┐      ┌───────────────┐
│   │               │      │               │      │               │      │               │
│   │  1) SIMPLIFIED│      │ 2) SELF-ATTN  │      │  3) CAUSAL    │      │ 4) MULTI-HEAD │
│   │  SELF-ATTN    │      │ WITH WEIGHTS  │      │  ATTENTION    │      │  ATTENTION    │
│   │               │      │               │      │               │      │               │
│   │      ✓        │ ───▶ │  ◀══ NOW ══▶  │ ───▶ │               │ ───▶ │               │
│   │   DONE!       │      │               │      │               │      │               │
│   └───────────────┘      └───────────────┘      └───────────────┘      └───────────────┘
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  WHAT'S NEW?                                                              ║ │
│  ║  ───────────                                                              ║ │
│  ║                                                                           ║ │
│  ║  BEFORE (simplified):                                                     ║ │
│  ║  • Used raw input vectors directly                                        ║ │
│  ║  • No learning - fixed computation                                        ║ │
│  ║                                                                           ║ │
│  ║  NOW (with trainable weights):                                            ║ │
│  ║  • Add W_query, W_key, W_value weight matrices                           ║ │
│  ║  • Model can LEARN what to attend to                                      ║ │
│  ║  • This is what's used in real LLMs!                                      ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.14: Computing Query, Key, Value Vectors

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    CREATING Q, K, V WITH WEIGHT MATRICES                         │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  INPUTS (6 tokens, 3 dimensions each):                                          │
│                                                                                 │
│       x⁽¹⁾          x⁽²⁾          x⁽³⁾          x⁽⁴⁾          x⁽⁵⁾          x⁽⁶⁾    │
│      "Your"      "journey"     "starts"      "with"        "one"        "step"  │
│     ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐ │
│     │ 0.4   │    │ 0.5   │    │ 0.5   │    │ 0.2   │    │ 0.8   │    │ 0.0   │ │
│     │ 0.1   │    │ 0.8   │    │ 0.8   │    │ 0.6   │    │ 0.2   │    │ 0.8   │ │
│     │ 0.8   │    │ 0.6   │    │ 0.6   │    │ 0.3   │    │ 0.1   │    │ 0.5   │ │
│     └───────┘    └───────┘    └───────┘    └───────┘    └───────┘    └───────┘ │
│                       │                                                         │
│                       │ We'll use x⁽²⁾ ("journey") as our example               │
│                       ▼                                                         │
│                                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                          │  │
│  │                    x⁽²⁾ = [0.5, 0.8, 0.6]                               │  │
│  │                              │                                           │  │
│  │           ┌──────────────────┼──────────────────┐                        │  │
│  │           │                  │                  │                        │  │
│  │           ▼                  ▼                  ▼                        │  │
│  │                                                                          │  │
│  │     ┌───────────┐      ┌───────────┐      ┌───────────┐                  │  │
│  │     │  W_query  │      │   W_key   │      │  W_value  │                  │  │
│  │     │   (3×2)   │      │   (3×2)   │      │   (3×2)   │                  │  │
│  │     │           │      │           │      │           │                  │  │
│  │     │  ┌─────┐  │      │  ┌─────┐  │      │  ┌─────┐  │                  │  │
│  │     │  │w w  │  │      │  │w w  │  │      │  │w w  │  │                  │  │
│  │     │  │w w  │  │      │  │w w  │  │      │  │w w  │  │                  │  │
│  │     │  │w w  │  │      │  │w w  │  │      │  │w w  │  │                  │  │
│  │     │  └─────┘  │      │  └─────┘  │      │  └─────┘  │                  │  │
│  │     └─────┬─────┘      └─────┬─────┘      └─────┬─────┘                  │  │
│  │           │                  │                  │                        │  │
│  │     x⁽²⁾ @ W_query     x⁽²⁾ @ W_key      x⁽²⁾ @ W_value                 │  │
│  │           │                  │                  │                        │  │
│  │           ▼                  ▼                  ▼                        │  │
│  │                                                                          │  │
│  │     ┌───────────┐      ┌───────────┐      ┌───────────┐                  │  │
│  │     │  QUERY    │      │   KEY     │      │  VALUE    │                  │  │
│  │     │   q⁽²⁾    │      │   k⁽²⁾    │      │   v⁽²⁾    │                  │  │
│  │     │           │      │           │      │           │                  │  │
│  │     │  ┌─────┐  │      │  ┌─────┐  │      │  ┌─────┐  │                  │  │
│  │     │  │ 0.4 │  │      │  │ 0.4 │  │      │  │ 0.3 │  │                  │  │
│  │     │  │ 1.4 │  │      │  │ 1.1 │  │      │  │ 1.0 │  │                  │  │
│  │     │  └─────┘  │      │  └─────┘  │      │  └─────┘  │                  │  │
│  │     └───────────┘      └───────────┘      └───────────┘                  │  │
│  │                                                                          │  │
│  │        (2D)               (2D)               (2D)                        │  │
│  │                                                                          │  │
│  └──────────────────────────────────────────────────────────────────────────┘  │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  THE LIBRARY ANALOGY: WHY Q, K, V?                                        ║ │
│  ║  ────────────────────────────────                                         ║ │
│  ║                                                                           ║ │
│  ║  🔍 QUERY (Q):  "What am I looking for?"                                  ║ │
│  ║               Like your search question in a library                      ║ │
│  ║                                                                           ║ │
│  ║  🏷️ KEY (K):    "What does this book contain?"                            ║ │
│  ║               Like labels/tags on books                                   ║ │
│  ║                                                                           ║ │
│  ║  📖 VALUE (V):  "What information does it provide?"                       ║ │
│  ║               The actual content of the book                              ║ │
│  ║                                                                           ║ │
│  ║  • Match QUERY against KEYS to find relevant books                        ║ │
│  ║  • Retrieve VALUES from matching books                                    ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  d_in, d_out = 3, 2                                                            │
│  W_query = nn.Parameter(torch.rand(d_in, d_out))                               │
│  W_key = nn.Parameter(torch.rand(d_in, d_out))                                 │
│  W_value = nn.Parameter(torch.rand(d_in, d_out))                               │
│                                                                                 │
│  query_2 = x_2 @ W_query  # tensor([0.4306, 1.4551])                           │
│  key_2 = x_2 @ W_key                                                           │
│  value_2 = x_2 @ W_value                                                       │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.15: Computing Attention Scores with Q and K

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    ATTENTION SCORES WITH Q AND K                                 │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  NOW: Dot product between QUERY and KEYS (not raw inputs!)                      │
│                                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                          │  │
│  │   INPUTS X                                                               │  │
│  │   ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐           │  │
│  │   │ 0.4   │ │ 0.5   │ │ 0.5   │ │ 0.2   │ │ 0.8   │ │ 0.0   │           │  │
│  │   │ 0.1   │ │ 0.8   │ │ 0.8   │ │ 0.6   │ │ 0.2   │ │ 0.8   │           │  │
│  │   │ 0.8   │ │ 0.6   │ │ 0.6   │ │ 0.3   │ │ 0.1   │ │ 0.5   │           │  │
│  │   └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘           │  │
│  │       │         │         │         │         │         │               │  │
│  │       ▼         ▼         ▼         ▼         ▼         ▼               │  │
│  │       @         @         @         @         @         @               │  │
│  │      W_k       W_k       W_k       W_k       W_k       W_k              │  │
│  │       │         │         │         │         │         │               │  │
│  │       ▼         ▼         ▼         ▼         ▼         ▼               │  │
│  │                                                                          │  │
│  │   KEYS K                                                                 │  │
│  │   ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐           │  │
│  │   │ 0.3   │ │ 0.4   │ │ 0.4   │ │ 0.3   │ │ 0.3   │ │ 0.3   │           │  │
│  │   │ 0.7   │ │ 1.1   │ │ 1.1   │ │ 1.0   │ │ 0.9   │ │ 0.7   │           │  │
│  │   └───────┘ └───────┘ └───────┘ └───────┘ └───────┘ └───────┘           │  │
│  │       ▲                                                                  │  │
│  │       │                                                                  │  │
│  │       │  DOT PRODUCT with query q⁽²⁾ = [0.4, 1.4]                       │  │
│  │       │                                                                  │  │
│  │       ▼                                                                  │  │
│  │                                                                          │  │
│  │   QUERY q⁽²⁾ = [0.4, 1.4]                                               │  │
│  │                                                                          │  │
│  │       │  q⁽²⁾ · k⁽¹⁾    q⁽²⁾ · k⁽²⁾    q⁽²⁾ · k⁽³⁾    ...               │  │
│  │       │                                                                  │  │
│  │       ▼                                                                  │  │
│  │                                                                          │  │
│  │   ATTENTION SCORES (ω)                                                   │  │
│  │   ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐           │  │
│  │   │ ω₂₁   │ │ ω₂₂   │ │ ω₂₃   │ │ ω₂₄   │ │ ω₂₅   │ │ ω₂₆   │           │  │
│  │   │ 1.27  │ │ 1.85  │ │ 1.81  │ │ 1.08  │ │ 0.56  │ │ 1.54  │           │  │
│  │   └───────┘ └───────┘ └───────┘ └───────┘ └───────┘ └───────┘           │  │
│  │                  ▲                                                       │  │
│  │                  │                                                       │  │
│  │             HIGHEST!                                                     │  │
│  │         (most relevant)                                                  │  │
│  │                                                                          │  │
│  └──────────────────────────────────────────────────────────────────────────┘  │
│                                                                                 │
│  KEY DIFFERENCE FROM SIMPLIFIED VERSION:                                        │
│  ────────────────────────────────────────                                       │
│                                                                                 │
│  BEFORE: score = input · input                                                  │
│  NOW:    score = (input @ W_q) · (input @ W_k) = query · key                   │
│                                                                                 │
│  The model can LEARN what makes a good query/key match!                         │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  keys = inputs @ W_key           # All keys: (6, 2)                            │
│  attn_scores_2 = query_2 @ keys.T # Scores for query 2: (6,)                   │
│  # tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])                    │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.16: Scale and Softmax

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    SCALED DOT-PRODUCT ATTENTION                                  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                          │  │
│  │   ATTENTION SCORES (ω)                                                   │  │
│  │   ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐           │  │
│  │   │ 1.27  │ │ 1.85  │ │ 1.81  │ │ 1.08  │ │ 0.56  │ │ 1.54  │           │  │
│  │   └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘           │  │
│  │       │         │         │         │         │         │               │  │
│  │       └─────────┴─────────┴─────────┴─────────┴─────────┘               │  │
│  │                              │                                           │  │
│  │                              ▼                                           │  │
│  │                                                                          │  │
│  │              ┌───────────────────────────────────┐                       │  │
│  │              │         SCALE BY √d_k             │                       │  │
│  │              │                                   │                       │  │
│  │              │      scores / √(head_dim)         │                       │  │
│  │              │                                   │                       │  │
│  │              │      If d_k = 2: divide by √2     │                       │  │
│  │              │                   = 1.41          │                       │  │
│  │              │                                   │                       │  │
│  │              └─────────────┬─────────────────────┘                       │  │
│  │                            │                                             │  │
│  │                            ▼                                             │  │
│  │                                                                          │  │
│  │   SCALED SCORES                                                          │  │
│  │   ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐           │  │
│  │   │ 0.90  │ │ 1.31  │ │ 1.28  │ │ 0.76  │ │ 0.40  │ │ 1.09  │           │  │
│  │   └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘           │  │
│  │       │         │         │         │         │         │               │  │
│  │       └─────────┴─────────┴─────────┴─────────┴─────────┘               │  │
│  │                              │                                           │  │
│  │                              ▼                                           │  │
│  │                                                                          │  │
│  │              ┌───────────────────────────────────┐                       │  │
│  │              │           SOFTMAX                 │                       │  │
│  │              │                                   │                       │  │
│  │              │        αᵢ = e^(scaled_ωᵢ)        │                       │  │
│  │              │             ───────────           │                       │  │
│  │              │             Σ e^(scaled_ωⱼ)       │                       │  │
│  │              │                                   │                       │  │
│  │              └─────────────┬─────────────────────┘                       │  │
│  │                            │                                             │  │
│  │                            ▼                                             │  │
│  │                                                                          │  │
│  │   ATTENTION WEIGHTS (α) - Sum to 1.0!                                    │  │
│  │   ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐           │  │
│  │   │ 0.15  │ │ 0.23  │ │ 0.22  │ │ 0.13  │ │ 0.09  │ │ 0.18  │           │  │
│  │   └───────┘ └───────┘ └───────┘ └───────┘ └───────┘ └───────┘           │  │
│  │                                                                          │  │
│  └──────────────────────────────────────────────────────────────────────────┘  │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  WHY SCALE BY √d_k?                                                       ║ │
│  ║  ─────────────────                                                        ║ │
│  ║                                                                           ║ │
│  ║  PROBLEM: Large embedding dimensions → Large dot products                 ║ │
│  ║           Large dot products → Extreme softmax values                     ║ │
│  ║           Extreme softmax → Tiny gradients → Slow/stuck training          ║ │
│  ║                                                                           ║ │
│  ║  EXAMPLE:                                                                 ║ │
│  ║  ────────                                                                 ║ │
│  ║  If d_k = 1000 and scores = [100, 101, 99, ...]                          ║ │
│  ║  softmax([100, 101, 99]) ≈ [0.27, 0.73, 0.001]  ← extreme!               ║ │
│  ║                                                                           ║ │
│  ║  After scaling by √1000 ≈ 31.6:                                          ║ │
│  ║  scores = [3.16, 3.19, 3.13, ...]                                        ║ │
│  ║  softmax([3.16, 3.19, 3.13]) ≈ [0.32, 0.34, 0.31]  ← reasonable!         ║ │
│  ║                                                                           ║ │
│  ║  This is why it's called "SCALED dot-product attention"!                  ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  d_k = keys.shape[-1]  # dimension of keys                                     │
│  attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)             │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.17: Computing Context Vector with Values

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    CONTEXT VECTOR FROM VALUES                                    │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  FINAL STEP: Apply attention weights to VALUES (not raw inputs!)               │
│                                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                          │  │
│  │   INPUTS X                                                               │  │
│  │   ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐           │  │
│  │   │ 0.4   │ │ 0.5   │ │ 0.5   │ │ 0.2   │ │ 0.8   │ │ 0.0   │           │  │
│  │   │ 0.1   │ │ 0.8   │ │ 0.8   │ │ 0.6   │ │ 0.2   │ │ 0.8   │           │  │
│  │   │ 0.8   │ │ 0.6   │ │ 0.6   │ │ 0.3   │ │ 0.1   │ │ 0.5   │           │  │
│  │   └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘           │  │
│  │       │         │         │         │         │         │               │  │
│  │       ▼         ▼         ▼         ▼         ▼         ▼               │  │
│  │       @         @         @         @         @         @               │  │
│  │      W_v       W_v       W_v       W_v       W_v       W_v              │  │
│  │       │         │         │         │         │         │               │  │
│  │       ▼         ▼         ▼         ▼         ▼         ▼               │  │
│  │                                                                          │  │
│  │   VALUES V (the CONTENT to retrieve)                                     │  │
│  │   ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐           │  │
│  │   │ v⁽¹⁾  │ │ v⁽²⁾  │ │ v⁽³⁾  │ │ v⁽⁴⁾  │ │ v⁽⁵⁾  │ │ v⁽⁶⁾  │           │  │
│  │   │ 0.3   │ │ 0.3   │ │ 0.3   │ │ 0.3   │ │ 0.3   │ │ 0.3   │           │  │
│  │   │ 0.9   │ │ 1.0   │ │ 1.0   │ │ 0.9   │ │ 0.7   │ │ 0.9   │           │  │
│  │   └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘           │  │
│  │       │         │         │         │         │         │               │  │
│  │       ▼         ▼         ▼         ▼         ▼         ▼               │  │
│  │       ×         ×         ×         ×         ×         ×               │  │
│  │      0.15      0.23      0.22      0.13      0.09      0.18             │  │
│  │                                                                          │  │
│  │   ATTENTION WEIGHTS (α)                                                  │  │
│  │                                                                          │  │
│  │       │         │         │         │         │         │               │  │
│  │       ▼         ▼         ▼         ▼         ▼         ▼               │  │
│  │                                                                          │  │
│  │   WEIGHTED VALUES                                                        │  │
│  │   ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐           │  │
│  │   │ 0.045 │ │ 0.069 │ │ 0.066 │ │ 0.039 │ │ 0.027 │ │ 0.054 │           │  │
│  │   │ 0.135 │ │ 0.230 │ │ 0.220 │ │ 0.117 │ │ 0.063 │ │ 0.162 │           │  │
│  │   └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘           │  │
│  │       │         │         │         │         │         │               │  │
│  │       └─────────┴─────────┴─────────┴─────────┴─────────┘               │  │
│  │                              │                                           │  │
│  │                           SUM                                            │  │
│  │                              │                                           │  │
│  │                              ▼                                           │  │
│  │                                                                          │  │
│  │                        ┌───────────┐                                     │  │
│  │                        │           │                                     │  │
│  │                z⁽²⁾ = │   0.31    │   CONTEXT VECTOR                    │  │
│  │                        │   0.82    │   (2 dimensions now!)               │  │
│  │                        │           │                                     │  │
│  │                        └───────────┘                                     │  │
│  │                                                                          │  │
│  └──────────────────────────────────────────────────────────────────────────┘  │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  KEY INSIGHT:                                                             ║ │
│  ║  ────────────                                                             ║ │
│  ║                                                                           ║ │
│  ║  Q and K determine HOW MUCH attention (the weights)                       ║ │
│  ║  V provides WHAT information to retrieve (the content)                    ║ │
│  ║                                                                           ║ │
│  ║  ┌───────────────────────────────────────────────────────────────────┐   ║ │
│  ║  │  Q × K^T  →  Attention Weights  →  HOW MUCH to attend             │   ║ │
│  ║  │      │                                    │                       │   ║ │
│  ║  │      └────────────────────────────────────┘                       │   ║ │
│  ║  │                         │                                         │   ║ │
│  ║  │               Attention Weights × V  →  WHAT to retrieve          │   ║ │
│  ║  └───────────────────────────────────────────────────────────────────┘   ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  context_vec_2 = attn_weights_2 @ values                                       │
│  # tensor([0.3061, 0.8210])                                                    │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.18: Complete Self-Attention Picture

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    COMPLETE SELF-ATTENTION WITH TRAINABLE WEIGHTS                │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│   INPUTS X (6 tokens × 3 dimensions)                                            │
│   ┌─────────────────────────────────────────────────────────────────────┐      │
│   │  Your   journey  starts   with    one    step                       │      │
│   │  [0.4]   [0.5]   [0.5]   [0.2]   [0.8]   [0.0]                      │      │
│   │  [0.1]   [0.8]   [0.8]   [0.6]   [0.2]   [0.8]  ← x⁽²⁾ highlighted  │      │
│   │  [0.8]   [0.6]   [0.6]   [0.3]   [0.1]   [0.5]                      │      │
│   └───────────────────────┬─────────────────────────────────────────────┘      │
│                           │                                                     │
│          ┌────────────────┼────────────────┐                                   │
│          │                │                │                                   │
│          ▼                ▼                ▼                                   │
│    ┌───────────┐    ┌───────────┐    ┌───────────┐                             │
│    │  W_query  │    │   W_key   │    │  W_value  │   TRAINABLE                 │
│    │   (3×2)   │    │   (3×2)   │    │   (3×2)   │   WEIGHTS!                  │
│    └─────┬─────┘    └─────┬─────┘    └─────┬─────┘                             │
│          │                │                │                                   │
│          ▼                ▼                ▼                                   │
│    ┌───────────┐    ┌───────────┐    ┌───────────┐                             │
│    │  Queries  │    │   Keys    │    │  Values   │                             │
│    │    Q      │    │    K      │    │    V      │                             │
│    │  (6×2)    │    │  (6×2)    │    │  (6×2)    │                             │
│    │           │    │           │    │           │                             │
│    │ [0.4,1.4] │    │ [0.4,1.1] │    │ [0.3,1.0] │ ← Row 2 shown              │
│    └─────┬─────┘    └─────┬─────┘    └───────────┘                             │
│          │                │                │                                   │
│          └───────┬────────┘                │                                   │
│                  │                         │                                   │
│                  ▼                         │                                   │
│    ┌──────────────────────────┐            │                                   │
│    │   Attention Scores       │            │                                   │
│    │   Q @ K^T                │            │                                   │
│    │   (6×2) @ (2×6) = (6×6)  │            │                                   │
│    └────────────┬─────────────┘            │                                   │
│                 │                          │                                   │
│                 ▼                          │                                   │
│    ┌──────────────────────────┐            │                                   │
│    │   Scale: ÷ √d_k          │            │                                   │
│    │   Softmax                │            │                                   │
│    └────────────┬─────────────┘            │                                   │
│                 │                          │                                   │
│                 ▼                          │                                   │
│    ┌──────────────────────────┐            │                                   │
│    │   Attention Weights      │            │                                   │
│    │   (6×6)                  │────────────┘                                   │
│    │                          │                                                │
│    │              Keys        │                                                │
│    │      Your jour star with one step                                         │
│    │ Your 0.19 0.16 0.16 0.15 0.17 0.15   │                                   │
│    │ jour 0.20 0.16 0.16 0.14 0.16 0.14   │                                   │
│    │ star 0.20 0.16 0.16 0.14 0.16 0.14   │                                   │
│    │ with 0.18 0.16 0.16 0.15 0.16 0.15   │                                   │
│    │  one 0.18 0.16 0.16 0.15 0.16 0.15   │                                   │
│    │ step 0.19 0.16 0.16 0.15 0.16 0.15   │                                   │
│    │                          │                                                │
│    └────────────┬─────────────┘                                                │
│                 │                                                               │
│                 │  @ Values                                                     │
│                 │  (6×6) @ (6×2)                                                │
│                 │                                                               │
│                 ▼                                                               │
│    ┌──────────────────────────┐                                                │
│    │   Context Vectors Z      │                                                │
│    │   (6×2)                  │                                                │
│    │                          │                                                │
│    │   z⁽¹⁾ = [0.30, 0.81]   │                                                │
│    │   z⁽²⁾ = [0.31, 0.82]   │ ← Context for "journey"                        │
│    │   z⁽³⁾ = [0.31, 0.82]   │                                                │
│    │   z⁽⁴⁾ = [0.29, 0.79]   │                                                │
│    │   z⁽⁵⁾ = [0.29, 0.79]   │                                                │
│    │   z⁽⁶⁾ = [0.30, 0.80]   │                                                │
│    │                          │                                                │
│    └──────────────────────────┘                                                │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  FORMULA:                                                                 ║ │
│  ║                                                                           ║ │
│  ║  Attention(Q, K, V) = softmax(Q × K^T / √d_k) × V                        ║ │
│  ║                                                                           ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

# SECTION 5: CAUSAL ATTENTION

## 📊 Figure 3.19: The Problem - Seeing Future Tokens

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    CAUSAL ATTENTION: HIDING THE FUTURE                           │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  THE PROBLEM:                                                                   │
│  ────────────                                                                   │
│  During TEXT GENERATION, we predict ONE word at a time.                         │
│  We shouldn't "cheat" by seeing words that haven't been generated yet!          │
│                                                                                 │
│                                                                                 │
│  EXAMPLE: Predicting what comes after "The cat"                                 │
│                                                                                 │
│       WRONG (standard attention):         RIGHT (causal attention):             │
│                                                                                 │
│       "The cat sat because it"            "The cat ___"                         │
│             │                                   │                               │
│             ▼                                   ▼                               │
│       Model sees "sat because it"         Model only sees "The cat"             │
│       when predicting "sat"               when predicting next word             │
│             │                                   │                               │
│             ▼                                   ▼                               │
│         CHEATING! 🚫                         FAIR! ✅                           │
│                                                                                 │
│                                                                                 │
│  ATTENTION WEIGHT MATRIX - BEFORE vs AFTER MASKING:                             │
│                                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                         │   │
│  │  BEFORE MASKING (can see everything):                                   │   │
│  │                                                                         │   │
│  │              Your  journey starts  with   one   step                    │   │
│  │         ┌─────────────────────────────────────────────┐                 │   │
│  │    Your │  0.20   0.16   0.16   0.14   0.16   0.14   │                  │   │
│  │ journey │  0.19   0.16   0.16   0.15   0.17   0.15   │  All positions   │   │
│  │  starts │  0.20   0.16   0.16   0.14   0.16   0.14   │  have non-zero   │   │
│  │    with │  0.18   0.16   0.16   0.15   0.16   0.15   │  weights         │   │
│  │     one │  0.18   0.16   0.16   0.15   0.16   0.15   │                  │   │
│  │    step │  0.19   0.16   0.16   0.15   0.16   0.15   │                  │   │
│  │         └─────────────────────────────────────────────┘                 │   │
│  │                                                                         │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│                                   │                                             │
│                                   │  APPLY CAUSAL MASK                          │
│                                   ▼                                             │
│                                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                         │   │
│  │  AFTER MASKING (can only see past + current):                           │   │
│  │                                                                         │   │
│  │              Your  journey starts  with   one   step                    │   │
│  │         ┌─────────────────────────────────────────────┐                 │   │
│  │    Your │  1.00   ────   ────   ────   ────   ────   │  Only sees self  │   │
│  │ journey │  0.55   0.45   ────   ────   ────   ────   │  Sees Your+self  │   │
│  │  starts │  0.38   0.31   0.31   ────   ────   ────   │                  │   │
│  │    with │  0.28   0.25   0.25   0.23   ────   ────   │                  │   │
│  │     one │  0.22   0.20   0.20   0.19   0.20   ────   │                  │   │
│  │    step │  0.19   0.17   0.17   0.15   0.17   0.15   │  Sees everything │   │
│  │         └─────────────────────────────────────────────┘                 │   │
│  │                                                                         │   │
│  │         ──── = Masked (cannot see)                                      │   │
│  │                                                                         │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  KEY INSIGHT:                                                             ║ │
│  ║  • Row 1 ("Your"): Can only see itself → weight = 1.0                     ║ │
│  ║  • Row 2 ("journey"): Can see "Your" + itself → weights split between them ║ │
│  ║  • Row 6 ("step"): Can see all previous words → normal attention          ║ │
│  ║                                                                           ║ │
│  ║  Each row still sums to 1.0 (after renormalization)!                      ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.20 & 3.21: Two Methods to Create the Mask

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    TWO METHODS TO CREATE CAUSAL MASK                             │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  METHOD 1: Mask AFTER Softmax (3 steps)          [Figure 3.20]            ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐         │
│  │ Attention Scores│      │ Attention       │      │ Masked Attention│         │
│  │  (unnormalized) │ ──▶  │ Weights         │ ──▶  │ (zeros above    │         │
│  │                 │      │ (softmax)       │      │  diagonal)      │         │
│  └─────────────────┘      └─────────────────┘      └────────┬────────┘         │
│                                                             │                  │
│                                                             ▼                  │
│                                                   ┌─────────────────┐          │
│                                                   │ Renormalized    │          │
│                                                   │ (rows sum to 1) │          │
│                                                   └─────────────────┘          │
│                                                                                 │
│  STEPS:                                                                         │
│  1. Apply softmax to get attention weights                                      │
│  2. Multiply by lower-triangular mask (zeros above diagonal)                    │
│  3. Renormalize each row to sum to 1                                            │
│                                                                                 │
│  CODE:                                                                          │
│  mask_simple = torch.tril(torch.ones(6, 6))  # Lower triangular                │
│  masked_simple = attn_weights * mask_simple                                    │
│  row_sums = masked_simple.sum(dim=-1, keepdim=True)                            │
│  masked_simple_norm = masked_simple / row_sums                                 │
│                                                                                 │
│                                                                                 │
│  ════════════════════════════════════════════════════════════════════════════  │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  METHOD 2: Mask BEFORE Softmax (2 steps) ✅ MORE EFFICIENT [Figure 3.21]  ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  ┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐         │
│  │ Attention Scores│      │ Masked Scores   │      │ Attention       │         │
│  │  (unnormalized) │ ──▶  │ (−∞ above       │ ──▶  │ Weights         │         │
│  │                 │      │  diagonal)      │      │ (softmax)       │         │
│  └─────────────────┘      └─────────────────┘      └─────────────────┘         │
│                                                                                 │
│  ONLY 2 STEPS! Why does this work?                                              │
│                                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                         │   │
│  │   MAGIC OF SOFTMAX:                                                     │   │
│  │                                                                         │   │
│  │   softmax(-∞) = e^(-∞) / sum = 0 / sum = 0                             │   │
│  │                                                                         │   │
│  │   So positions with -∞ automatically become 0 after softmax!            │   │
│  │   No need for separate renormalization step!                            │   │
│  │                                                                         │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│  EXAMPLE:                                                                       │
│                                                                                 │
│  BEFORE MASK:              AFTER MASK (-∞):          AFTER SOFTMAX:             │
│                                                                                 │
│  ┌─────────────────┐       ┌─────────────────┐       ┌─────────────────┐        │
│  │ 0.29 0.47 0.46  │       │ 0.29  -∞   -∞   │       │ 1.00 0.00 0.00  │        │
│  │ 0.26 0.17 0.17  │  ──▶  │ 0.26 0.17  -∞   │  ──▶  │ 0.52 0.48 0.00  │        │
│  │ 0.34 0.13 0.13  │       │ 0.34 0.13 0.13  │       │ 0.39 0.31 0.31  │        │
│  └─────────────────┘       └─────────────────┘       └─────────────────┘        │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  # Create upper triangular mask (1s above diagonal)                            │
│  mask = torch.triu(torch.ones(6, 6), diagonal=1)                               │
│                                                                                 │
│  # Fill those positions with -infinity                                         │
│  masked = attn_scores.masked_fill(mask.bool(), -torch.inf)                     │
│                                                                                 │
│  # Apply softmax - masked positions automatically become 0!                    │
│  attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)           │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.22: Dropout Mask

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    DROPOUT: ADDITIONAL REGULARIZATION                            │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  PURPOSE: Prevent overfitting by randomly "dropping" some attention weights     │
│           during TRAINING (disabled during inference)                           │
│                                                                                 │
│                                                                                 │
│  ┌───────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                           │ │
│  │   CAUSAL MASK                       DROPOUT MASK                          │ │
│  │   (from previous step)              (random zeros)                        │ │
│  │                                                                           │ │
│  │      Your jour star with one step       Random pattern                    │ │
│  │   ┌───────────────────────────────┐  ┌───────────────────────────────┐   │ │
│  │   │ 1.00  ──   ──   ──   ──   ── │  │  1    0    1    1    0    1  │    │ │
│  │   │ 0.55 0.45  ──   ──   ──   ── │  │  1    1    0    1    1    0  │    │ │
│  │   │ 0.38 0.31 0.31  ──   ──   ── │  │  0    1    1    0    1    1  │    │ │
│  │   │ 0.28 0.25 0.25 0.23  ──   ── │  │  1    0    1    1    0    1  │    │ │
│  │   │ 0.22 0.20 0.20 0.19 0.20  ── │  │  1    1    0    1    1    0  │    │ │
│  │   │ 0.19 0.17 0.17 0.15 0.17 0.15│  │  0    1    1    1    1    1  │    │ │
│  │   └───────────────────────────────┘  └───────────────────────────────┘   │ │
│  │                                                                           │ │
│  │                        │                          │                       │ │
│  │                        └────────────┬─────────────┘                       │ │
│  │                                     │                                     │ │
│  │                                     ×                                     │ │
│  │                                     │                                     │ │
│  │                                     ▼                                     │ │
│  │                                                                           │ │
│  │   FINAL ATTENTION WEIGHTS                                                 │ │
│  │   (causal + dropout)                                                      │ │
│  │                                                                           │ │
│  │      Your jour star with one step                                         │ │
│  │   ┌───────────────────────────────┐                                       │ │
│  │   │ 2.00  ──   ──   ──   ──   ── │  ← Rescaled!                          │ │
│  │   │ 1.10 0.90  ──   ──   ──   ── │    (to maintain same total)           │ │
│  │   │ ──   0.62 0.62  ──   ──   ── │                                       │ │
│  │   │ 0.56 ──   0.50 0.46  ──   ── │                                       │ │
│  │   │ 0.44 0.40 ──   0.38 0.40  ── │                                       │ │
│  │   │ ──   0.34 0.34 0.30 0.34 0.30│                                       │ │
│  │   └───────────────────────────────┘                                       │ │
│  │                                                                           │ │
│  │   ── = zeroed (either by causal mask OR dropout)                          │ │
│  │                                                                           │ │
│  └───────────────────────────────────────────────────────────────────────────┘ │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  WHY RESCALE?                                                             ║ │
│  ║  ────────────                                                             ║ │
│  ║                                                                           ║ │
│  ║  If dropout rate = 50%, half the weights are zeroed.                      ║ │
│  ║  To keep the same TOTAL attention, remaining weights are scaled by 2.     ║ │
│  ║                                                                           ║ │
│  ║  Scale factor = 1 / (1 - dropout_rate) = 1 / 0.5 = 2                     ║ │
│  ║                                                                           ║ │
│  ║  This way, expected total remains the same during training and inference! ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  dropout = torch.nn.Dropout(0.5)  # 50% dropout rate                           │
│  attn_weights = dropout(attn_weights)                                          │
│                                                                                 │
│  NOTE: In real training, use lower dropout (0.1 or 0.2)                        │
│        Dropout is DISABLED during inference (model.eval())                     │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.23: Progress - Causal Attention Done!

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    OUR PROGRESS                                                  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│   ┌───────────────┐      ┌───────────────┐      ┌───────────────┐      ┌───────────────┐
│   │               │      │               │      │               │      │               │
│   │  1) SIMPLIFIED│      │ 2) SELF-ATTN  │      │  3) CAUSAL    │      │ 4) MULTI-HEAD │
│   │  SELF-ATTN    │      │ WITH WEIGHTS  │      │  ATTENTION    │      │  ATTENTION    │
│   │               │      │               │      │               │      │               │
│   │      ✓        │ ───▶ │      ✓        │ ───▶ │      ✓        │ ───▶ │  ◀══ NEXT ══▶ │
│   │   DONE!       │      │   DONE!       │      │   DONE!       │      │               │
│   └───────────────┘      └───────────────┘      └───────────────┘      └───────────────┘
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  WHAT WE'VE ADDED:                                                        ║ │
│  ║  ─────────────────                                                        ║ │
│  ║                                                                           ║ │
│  ║  1. Simplified: Basic dot-product attention                               ║ │
│  ║  2. With weights: Trainable W_query, W_key, W_value                       ║ │
│  ║  3. Causal: Mask to hide future + dropout for regularization              ║ │
│  ║                                                                           ║ │
│  ║  NEXT: Multi-head attention (final version for GPT!)                      ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

# SECTION 6: MULTI-HEAD ATTENTION

## 📊 Figure 3.24: Multi-Head Attention Structure

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    MULTI-HEAD ATTENTION (2 Heads Example)                        │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  WHY MULTIPLE HEADS?                                                            │
│  ───────────────────                                                            │
│  One head can only capture ONE type of pattern.                                 │
│  Multiple heads can capture DIFFERENT relationships!                            │
│                                                                                 │
│  • Head 1: Subject-verb relationships ("cat" → "sat")                           │
│  • Head 2: Pronoun references ("it" → "cat")                                    │
│  • Head 3: Adjacent word context                                                │
│  • Head 4: Long-range dependencies                                              │
│  • etc.                                                                         │
│                                                                                 │
│  ┌──────────────────────────────────────────────────────────────────────────┐  │
│  │                                                                          │  │
│  │   INPUTS X (6 tokens × 3 dimensions)                                     │  │
│  │   ┌─────────────────────────────────────────────────────────────────┐   │  │
│  │   │  Your   journey  starts   with    one    step                   │   │  │
│  │   │  [0.4]   [0.5]   [0.5]   [0.2]   [0.8]   [0.0]                 │   │  │
│  │   │  [0.1]   [0.8]   [0.8]   [0.6]   [0.2]   [0.8]                 │   │  │
│  │   │  [0.8]   [0.6]   [0.6]   [0.3]   [0.1]   [0.5]                 │   │  │
│  │   └───────────────────────────┬─────────────────────────────────────┘   │  │
│  │                               │                                         │  │
│  │              ┌────────────────┴────────────────┐                        │  │
│  │              │                                 │                        │  │
│  │              ▼                                 ▼                        │  │
│  │   ╔═══════════════════════╗       ╔═══════════════════════╗            │  │
│  │   ║      HEAD 1           ║       ║      HEAD 2           ║            │  │
│  │   ║                       ║       ║                       ║            │  │
│  │   ║  ┌─────┐┌─────┐┌─────┐║       ║  ┌─────┐┌─────┐┌─────┐║            │  │
│  │   ║  │W_q1 ││W_k1 ││W_v1 │║       ║  │W_q2 ││W_k2 ││W_v2 │║            │  │
│  │   ║  └──┬──┘└──┬──┘└──┬──┘║       ║  └──┬──┘└──┬──┘└──┬──┘║            │  │
│  │   ║     │      │      │   ║       ║     │      │      │   ║            │  │
│  │   ║     ▼      ▼      ▼   ║       ║     ▼      ▼      ▼   ║            │  │
│  │   ║    Q1     K1     V1   ║       ║    Q2     K2     V2   ║            │  │
│  │   ║     │      │      │   ║       ║     │      │      │   ║            │  │
│  │   ║     └──┬───┘      │   ║       ║     └──┬───┘      │   ║            │  │
│  │   ║        │          │   ║       ║        │          │   ║            │  │
│  │   ║        ▼          │   ║       ║        ▼          │   ║            │  │
│  │   ║  ┌──────────┐     │   ║       ║  ┌──────────┐     │   ║            │  │
│  │   ║  │Attention │     │   ║       ║  │Attention │     │   ║            │  │
│  │   ║  │ Weights  │     │   ║       ║  │ Weights  │     │   ║            │  │
│  │   ║  │(+mask)   │     │   ║       ║  │(+mask)   │     │   ║            │  │
│  │   ║  └────┬─────┘     │   ║       ║  └────┬─────┘     │   ║            │  │
│  │   ║       │           │   ║       ║       │           │   ║            │  │
│  │   ║       └─────┬─────┘   ║       ║       └─────┬─────┘   ║            │  │
│  │   ║             │         ║       ║             │         ║            │  │
│  │   ║             ▼         ║       ║             ▼         ║            │  │
│  │   ║      ┌───────────┐    ║       ║      ┌───────────┐    ║            │  │
│  │   ║      │ Context   │    ║       ║      │ Context   │    ║            │  │
│  │   ║      │ Vectors   │    ║       ║      │ Vectors   │    ║            │  │
│  │   ║      │    Z1     │    ║       ║      │    Z2     │    ║            │  │
│  │   ║      │  (6×2)    │    ║       ║      │  (6×2)    │    ║            │  │
│  │   ║      └─────┬─────┘    ║       ║      └─────┬─────┘    ║            │  │
│  │   ╚════════════╪══════════╝       ╚════════════╪══════════╝            │  │
│  │                │                               │                        │  │
│  │                └───────────────┬───────────────┘                        │  │
│  │                                │                                         │  │
│  │                                ▼                                         │  │
│  │                     ┌────────────────────┐                               │  │
│  │                     │    CONCATENATE     │                               │  │
│  │                     │    [Z1 | Z2]       │                               │  │
│  │                     │    (6×4)           │                               │  │
│  │                     └─────────┬──────────┘                               │  │
│  │                               │                                          │  │
│  │                               ▼                                          │  │
│  │                     ┌────────────────────┐                               │  │
│  │                     │  COMBINED OUTPUT   │                               │  │
│  │                     │       Z            │                               │  │
│  │                     │     (6×4)          │                               │  │
│  │                     └────────────────────┘                               │  │
│  │                                                                          │  │
│  └──────────────────────────────────────────────────────────────────────────┘  │
│                                                                                 │
│  FORMULA: d_out = num_heads × head_dim                                         │
│           4 = 2 × 2                                                             │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.25: Concatenation of Heads

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    CONCATENATING HEAD OUTPUTS                                    │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  EXAMPLE: 2 heads, each producing (6 tokens × 2 dimensions)                     │
│                                                                                 │
│                                                                                 │
│     HEAD 1 OUTPUT                    HEAD 2 OUTPUT                              │
│         Z1                               Z2                                     │
│                                                                                 │
│     ┌─────────────┐                  ┌─────────────┐                           │
│     │  z1⁽¹⁾      │                  │  z2⁽¹⁾      │                           │
│     │ [-0.7,-0.1] │                  │ [0.5, 0.1]  │                           │
│     ├─────────────┤                  ├─────────────┤                           │
│     │  z1⁽²⁾      │                  │  z2⁽²⁾      │                           │
│     │ [-0.6, 0.0] │                  │ [0.6, 0.3]  │                           │
│     ├─────────────┤                  ├─────────────┤                           │
│     │  z1⁽³⁾      │                  │  z2⁽³⁾      │                           │
│     │ [-0.6,-0.1] │                  │ [0.6, 0.4]  │                           │
│     ├─────────────┤                  ├─────────────┤                           │
│     │  z1⁽⁴⁾      │                  │  z2⁽⁴⁾      │                           │
│     │ [-0.6,-0.1] │                  │ [0.5, 0.4]  │                           │
│     ├─────────────┤   CONCATENATE    ├─────────────┤                           │
│     │  z1⁽⁵⁾      │ ──────────────▶  │  z2⁽⁵⁾      │                           │
│     │ [-0.6,-0.1] │                  │ [0.5, 0.3]  │                           │
│     ├─────────────┤                  ├─────────────┤                           │
│     │  z1⁽⁶⁾      │                  │  z2⁽⁶⁾      │                           │
│     │ [-0.5,-0.1] │                  │ [0.5, 0.3]  │                           │
│     └─────────────┘                  └─────────────┘                           │
│                                                                                 │
│         (6 × 2)                          (6 × 2)                                │
│                                                                                 │
│                              │                                                  │
│                              ▼                                                  │
│                                                                                 │
│                    CONCATENATED OUTPUT Z                                        │
│                                                                                 │
│     ┌─────────────────────────────────────────┐                                │
│     │  z⁽¹⁾ = [-0.7, -0.1,  0.5,  0.1]       │                                │
│     │  z⁽²⁾ = [-0.6,  0.0,  0.6,  0.3]       │                                │
│     │  z⁽³⁾ = [-0.6, -0.1,  0.6,  0.4]       │                                │
│     │  z⁽⁴⁾ = [-0.6, -0.1,  0.5,  0.4]       │                                │
│     │  z⁽⁵⁾ = [-0.6, -0.1,  0.5,  0.3]       │  ← 5th row example             │
│     │  z⁽⁶⁾ = [-0.5, -0.1,  0.5,  0.3]       │                                │
│     └─────────────────────────────────────────┘                                │
│                                                                                 │
│                        (6 × 4)                                                  │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  DIMENSION CALCULATION:                                                   ║ │
│  ║                                                                           ║ │
│  ║  d_out = num_heads × head_dim                                            ║ │
│  ║    4   =     2     ×    2                                                 ║ │
│  ║                                                                           ║ │
│  ║  Each head produces 2D context vectors                                    ║ │
│  ║  2 heads concatenated = 4D final output                                   ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│  CODE:                                                                          │
│  ─────                                                                          │
│  # In MultiHeadAttentionWrapper                                                │
│  def forward(self, x):                                                         │
│      return torch.cat([head(x) for head in self.heads], dim=-1)               │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 📊 Figure 3.26: Efficient Implementation (Split Method)

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    TWO WAYS TO IMPLEMENT MULTI-HEAD                              │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  METHOD 1: WRAPPER (Simple but Slower)                                    ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│     INPUTS X                                                                    │
│         │                                                                       │
│         │                                                                       │
│    ┌────┴─────────────────────────────┐                                        │
│    │                                  │                                        │
│    ▼                                  ▼                                        │
│    ┌──────────┐                  ┌──────────┐                                  │
│    │  W_q1    │                  │  W_q2    │   TWO SEPARATE                   │
│    │  (3×2)   │                  │  (3×2)   │   WEIGHT MATRICES                │
│    └────┬─────┘                  └────┬─────┘                                  │
│         │                             │                                        │
│     X @ W_q1                      X @ W_q2      TWO MATRIX                     │
│         │                             │         MULTIPLICATIONS!              │
│         ▼                             ▼         (expensive)                   │
│    ┌──────────┐                  ┌──────────┐                                  │
│    │    Q1    │                  │    Q2    │                                  │
│    │  (6×2)   │                  │  (6×2)   │                                  │
│    └──────────┘                  └──────────┘                                  │
│                                                                                 │
│    ❌ SLOW: Need separate matrix multiplication for each head                   │
│                                                                                 │
│                                                                                 │
│  ════════════════════════════════════════════════════════════════════════════  │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  METHOD 2: SPLIT (Complex but Faster) ✅ USED IN GPT!                     ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│     INPUTS X                                                                    │
│         │                                                                       │
│         │                                                                       │
│         ▼                                                                       │
│    ┌──────────────┐                                                            │
│    │     W_q      │   ONE BIG                                                  │
│    │    (3×4)     │   WEIGHT MATRIX                                            │
│    │              │   (combines W_q1 and W_q2)                                 │
│    └───────┬──────┘                                                            │
│            │                                                                    │
│        X @ W_q         ONE MATRIX                                              │
│            │           MULTIPLICATION!                                         │
│            ▼           (efficient)                                             │
│    ┌──────────────┐                                                            │
│    │      Q       │                                                            │
│    │    (6×4)     │                                                            │
│    └───────┬──────┘                                                            │
│            │                                                                    │
│            │  .view(batch, seq, num_heads, head_dim)                           │
│            │  .transpose(1, 2)                                                 │
│            │                                                                    │
│            ▼                                                                    │
│    ┌──────────────────────────────────┐                                        │
│    │   Q reshaped to (batch, 2, 6, 2) │                                        │
│    │                                  │                                        │
│    │   ┌──────────┐  ┌──────────┐    │                                        │
│    │   │    Q1    │  │    Q2    │    │   SAME RESULT                          │
│    │   │  (6×2)   │  │  (6×2)   │    │   as Method 1!                         │
│    │   └──────────┘  └──────────┘    │                                        │
│    └──────────────────────────────────┘                                        │
│                                                                                 │
│    ✅ FAST: One matrix multiplication, then reshape                            │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║  THE KEY OPERATIONS:                                                      ║ │
│  ║  ───────────────────                                                      ║ │
│  ║                                                                           ║ │
│  ║  1. .view(b, seq, num_heads, head_dim)                                   ║ │
│  ║     → Split d_out into (num_heads × head_dim)                            ║ │
│  ║     → Shape: (b, 6, 4) → (b, 6, 2, 2)                                    ║ │
│  ║                                                                           ║ │
│  ║  2. .transpose(1, 2)                                                      ║ │
│  ║     → Move heads dimension before sequence                                ║ │
│  ║     → Shape: (b, 6, 2, 2) → (b, 2, 6, 2)                                 ║ │
│  ║                                                                           ║ │
│  ║  3. Batched matrix multiply                                               ║ │
│  ║     → Compute attention for ALL heads in parallel!                        ║ │
│  ║                                                                           ║ │
│  ║  4. .transpose(1, 2).contiguous().view(...)                              ║ │
│  ║     → Combine heads back together                                         ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

# SECTION 7: COMPLETE CODE & SUMMARY

## 📝 Complete MultiHeadAttention Code

```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention with Causal Mask
    ─────────────────────────────────────
    This is the FINAL version used in GPT-like models!
    """
    
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        
        # Validate: d_out must be divisible by num_heads
        assert d_out % num_heads == 0, "d_out must be divisible by num_heads"
        
        # Save configuration
        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # e.g., 768 // 12 = 64
        
        # STEP 1 weights: Q, K, V projections (one big matrix each)
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        
        # STEP 6 weights: Output projection
        self.out_proj = nn.Linear(d_out, d_out)
        
        # Dropout and causal mask
        self.dropout = nn.Dropout(dropout)
        self.register_buffer(
            "mask",
            torch.triu(torch.ones(context_length, context_length), diagonal=1)
        )
    
    def forward(self, x):
        b, num_tokens, d_in = x.shape
        
        # ══════════ STEP 1: PROJECT ══════════
        # Create Q, K, V via linear projection
        Q = self.W_query(x)  # (b, seq, d_out)
        K = self.W_key(x)
        V = self.W_value(x)
        
        # ══════════ STEP 2: SPLIT INTO HEADS ══════════
        # Reshape: (b, seq, d_out) → (b, seq, num_heads, head_dim)
        Q = Q.view(b, num_tokens, self.num_heads, self.head_dim)
        K = K.view(b, num_tokens, self.num_heads, self.head_dim)
        V = V.view(b, num_tokens, self.num_heads, self.head_dim)
        
        # ══════════ STEP 3: TRANSPOSE ══════════
        # (b, seq, heads, head_dim) → (b, heads, seq, head_dim)
        Q = Q.transpose(1, 2)
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)
        
        # ══════════ STEP 4: ATTENTION ══════════
        # 4a. Compute attention scores: Q @ K^T
        attn_scores = Q @ K.transpose(2, 3)  # (b, heads, seq, seq)
        
        # 4b. Apply causal mask
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
        attn_scores.masked_fill_(mask_bool, -torch.inf)
        
        # 4c. Scale and softmax
        attn_scores = attn_scores / (self.head_dim ** 0.5)
        attn_weights = torch.softmax(attn_scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # 4d. Apply attention to values
        context = attn_weights @ V  # (b, heads, seq, head_dim)
        
        # ══════════ STEP 5: CONCAT HEADS ══════════
        # (b, heads, seq, head_dim) → (b, seq, heads, head_dim) → (b, seq, d_out)
        context = context.transpose(1, 2)
        context = context.contiguous().view(b, num_tokens, self.d_out)
        
        # ══════════ STEP 6: OUTPUT PROJECTION ══════════
        output = self.out_proj(context)
        
        return output
```

---

## 📊 Summary: The 4 Attention Variants

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                         FINAL SUMMARY: 4 ATTENTION TYPES                         │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐   │
│  │  TYPE              │ FORMULA                        │ PURPOSE            │   │
│  ├─────────────────────────────────────────────────────────────────────────┤   │
│  │  1. Simplified     │ softmax(X @ X^T) @ X           │ Understand concept │   │
│  │                    │                                │                    │   │
│  │  2. With Weights   │ softmax(Q @ K^T / √d) @ V      │ Trainable learning │   │
│  │                    │ where Q=XW_q, K=XW_k, V=XW_v   │                    │   │
│  │                    │                                │                    │   │
│  │  3. Causal         │ Same as #2 + causal mask       │ Text generation    │   │
│  │                    │ + dropout                      │ (no future peek)   │   │
│  │                    │                                │                    │   │
│  │  4. Multi-Head     │ Concat(head_1, ..., head_h)    │ Multiple patterns  │   │
│  │                    │ where each head = Causal Attn  │ (GPT uses this!)   │   │
│  └─────────────────────────────────────────────────────────────────────────┘   │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║                          KEY FORMULAS                                     ║ │
│  ╠═══════════════════════════════════════════════════════════════════════════╣ │
│  ║                                                                           ║ │
│  ║  d_model = num_heads × head_dim                                          ║ │
│  ║    768   =    12     ×    64                                              ║ │
│  ║                                                                           ║ │
│  ║  Attention(Q, K, V) = softmax(Q × K^T / √d_k) × V                        ║ │
│  ║                                                                           ║ │
│  ║  MultiHead = Concat(head_1, ..., head_h) × W_output                      ║ │
│  ║                                                                           ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
│                                                                                 │
│  ╔═══════════════════════════════════════════════════════════════════════════╗ │
│  ║                        GPT-2 CONFIGURATIONS                               ║ │
│  ╠═══════════════════════════════════════════════════════════════════════════╣ │
│  ║                                                                           ║ │
│  ║  │ Model      │ d_model │ num_heads │ head_dim │ context_len │           ║ │
│  ║  ├────────────┼─────────┼───────────┼──────────┼─────────────┤           ║ │
│  ║  │ GPT-2 S    │   768   │    12     │    64    │    1024     │           ║ │
│  ║  │ GPT-2 M    │  1024   │    16     │    64    │    1024     │           ║ │
│  ║  │ GPT-2 L    │  1280   │    20     │    64    │    1024     │           ║ │
│  ║  │ GPT-2 XL   │  1600   │    25     │    64    │    1024     │           ║ │
│  ║  │ GPT-3      │ 12288   │    96     │   128    │    2048     │           ║ │
│  ║                                                                           ║ │
│  ╚═══════════════════════════════════════════════════════════════════════════╝ │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 🎯 FINAL CHECKLIST

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                         ✅ WHAT YOU SHOULD REMEMBER                              │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  □ The 4 types: Simplified → With Weights → Causal → Multi-Head                │
│                                                                                 │
│  □ The formula: d_model = num_heads × head_dim                                 │
│                                                                                 │
│  □ The 6 steps: Project → Split → Transpose → Attention → Concat → Project     │
│                                                                                 │
│  □ WHY Q, K, V: Query finds what to search, Key matches, Value provides content│
│                                                                                 │
│  □ WHY scale: Prevent extreme softmax values → better gradients                │
│                                                                                 │
│  □ WHY causal mask: No cheating by seeing future during text generation        │
│                                                                                 │
│  □ WHY multiple heads: Capture different relationship patterns                 │
│                                                                                 │
│  □ The key operations: .view(), .transpose(), @, .contiguous()                 │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘

                          🚀 Good luck with your studies! 🚀
```

---

**Created for Pascal - Based on Chapter 3 of "Build a Large Language Model From Scratch" by Sebastian Raschka**
